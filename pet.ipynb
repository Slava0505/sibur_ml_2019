{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:11:53.916714Z",
     "start_time": "2019-10-14T03:11:53.903793Z"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "plt.style.use(\"bmh\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import pathlib\n",
    "# catboost.varsion == 0.17.5\n",
    "from catboost import cv, Pool, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters,EfficientFCParameters, MinimalFCParameters\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "import tsfresh\n",
    "from  few import FEW\n",
    "from sklearn.linear_model import HuberRegressor, LinearRegression, Lasso, Lars\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"data/\")\n",
    "RS = 289475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T03:11:54.107142Z",
     "start_time": "2019-10-14T03:11:54.018430Z"
    }
   },
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(DATA_DIR.joinpath(\"pet_target_train.csv\"), index_col=\"date\", parse_dates=[\"date\"])\n",
    "test_target = pd.read_csv(DATA_DIR.joinpath(\"pet_test_timestamps.csv\"), index_col=\"date\", parse_dates=[\"date\"])\n",
    "daily = pd.read_csv(DATA_DIR.joinpath(\"pet_daily.csv\"), index_col=\"date\", parse_dates=[\"date\"])\n",
    "weekly = pd.read_csv(DATA_DIR.joinpath(\"pet_weekly.csv\"), index_col=\"date\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name in ['brent', 'USDCNY']:\n",
    "    daily[name] = daily[name+'_close']\n",
    "    daily[name+'_diff1'] = daily[name+'_close']/daily[name+'_open']  \n",
    "    daily[name+'_diff2'] = daily[name+'_max']/daily[name+'_min']\n",
    "    daily[name+'_std'] = daily[[name+'_close', name+'_open', name+'_max', name+'_min']].std(axis = 1)\n",
    "    daily[name+'_delta_2'] = daily[name]/daily[name].shift(2)\n",
    "    daily[name+'_delta_10'] = daily[name]/daily[name].shift(10)\n",
    "    daily[name+'_delta_30'] = daily[name]/daily[name].shift(30)\n",
    "    daily[name+'_delta_60'] = daily[name]/daily[name].shift(60)   \n",
    "    \n",
    "    daily = daily.drop([name+'_close', name+'_open', name+'_max', name+'_min'], axis =1)\n",
    "\n",
    "for col in ['paraxylene_CHN_USD', 'paraxylene_RT_USD', 'paraxylene_SEA_USD', 'pta_NEA_USD', 'ethylene_glycol_EU_EUR', 'ethylene_glycol_CHN_USD']:\n",
    "    weekly[col+'_delta_7'] = weekly[col]/weekly[col].shift(1)\n",
    "    weekly[col+'_delta_14'] = weekly[col]/weekly[col].shift(2)\n",
    "    weekly[col+'_delta_28'] = weekly[col]/weekly[col].shift(4)\n",
    "    weekly[col+'_delta_56'] = weekly[col]/weekly[col].shift(8)\n",
    "    \n",
    "weekly['paraxylene_diff1'] = weekly['paraxylene_CHN_USD']/weekly['paraxylene_RT_USD']\n",
    "weekly['paraxylene_diff2'] = weekly['paraxylene_CHN_USD']/weekly['paraxylene_SEA_USD']\n",
    "weekly['paraxylene_diff3'] = weekly['paraxylene_RT_USD']/weekly['paraxylene_SEA_USD']\n",
    "weekly['ethylene_glycol_diff1'] = weekly['ethylene_glycol_EU_EUR']/weekly['ethylene_glycol_CHN_USD']\n",
    "\n",
    "\n",
    "dfts = daily.resample(\"D\").mean()\n",
    "wfts = weekly.resample(\"D\").ffill()\n",
    "fts = dfts.join(wfts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Описание логики\n",
    "# Получается из всех дат беру только даты с днем==9, генерю по ним фичи, изменяю день в данных с 9го на 1ое, для \n",
    "# Последующего склеивания с таргетом. Склеивааю с .shift(1)\n",
    "\n",
    "def data_to_imp_columns(data, fdr_level = 5):\n",
    "    '''Функция принимает данные, склеивает их с таргетом с необходимым сдвигом,\n",
    "    и записывает отфильтрованные tsfresh'ем колонки'''\n",
    "    data.index = data.index.map(lambda x: pd.to_datetime('-'.join([str(x.year), str(x.month), '1'])))\n",
    "    data = train_target.join(data.shift(1))\n",
    "    x, y = data.drop(['pet'],axis = 1), data['pet']\n",
    "    columns = list(tsfresh.select_features(x.fillna(method = 'pad').fillna(data.mean()).dropna(axis = 1),  y, fdr_level=fdr_level).columns)\n",
    "    return columns\n",
    "\n",
    "columns = fts.columns\n",
    "settings = ComprehensiveFCParameters()\n",
    "\n",
    "all_data = []\n",
    "for col in columns:\n",
    "    for shift in [5, 10, 20, 30, 50, 70, 100, 182, 365, 730]:\n",
    "        # роллинг\n",
    "        df_shift, y = make_forecasting_frame(fts[col], kind=col, max_timeshift=shift, rolling_direction=1)\n",
    "        # берем только данные на 9й день месяца\n",
    "        df_shift = df_shift[df_shift['id'].apply(lambda x: pd.to_datetime(x).day)==9]\n",
    "        # генерим фичи\n",
    "        extracted_data = extract_features(df_shift, column_value  = 'value',column_id ='id', default_fc_parameters=settings)\n",
    "        # переименовываем фичи в соответствии с измененной колонкой и периодом\n",
    "        extracted_data.rename(lambda x: \"-\".join([x, str(shift)+'D', col]), axis=1, inplace=True)\n",
    "        # базовый feature selecting - откидывание совсем ненужных фич, констант и тд\n",
    "        extracted_data = extracted_data[data_to_imp_columns(extracted_data, 5)]\n",
    "        all_data.append(extracted_data)\n",
    "\n",
    "\n",
    "# Сбор нагенеренных фич\n",
    "data = pd.concat(all_data,axis = 1)\n",
    "# Переименовывание даты с 9го числа на 1ое для склеивания с таргетом\n",
    "data.index = data.index.map(lambda x: pd.to_datetime('-'.join([str(x.year), str(x.month), '1'])))\n",
    "\n",
    "# Чтобы data.shift работал корректно и для даты '2019-07-01' сдвигались предыдущие значения\n",
    "data.loc[pd.to_datetime('2019-07-01')] = None\n",
    "\n",
    "# Склеивание со сдвигом. \n",
    "train = train_target.join(data.shift(1))\n",
    "test = test_target.join(data.shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгружая исходные данные, ничего не удаляю\n",
    "train_target = pd.read_csv(DATA_DIR.joinpath(\"pet_target_train.csv\"), index_col=\"date\", parse_dates=[\"date\"])\n",
    "test_target = pd.read_csv(DATA_DIR.joinpath(\"pet_test_timestamps.csv\"), index_col=\"date\", parse_dates=[\"date\"])\n",
    "daily = pd.read_csv(DATA_DIR.joinpath(\"pet_daily.csv\"), index_col=\"date\", parse_dates=[\"date\"])\n",
    "weekly = pd.read_csv(DATA_DIR.joinpath(\"pet_weekly.csv\"), index_col=\"date\", parse_dates=[\"date\"])\n",
    "\n",
    "for name in ['brent', 'USDCNY']:\n",
    "    daily[name] = daily[name+'_close']\n",
    "    daily[name+'_diff1'] = daily[name+'_close']/daily[name+'_open']  \n",
    "    daily[name+'_diff2'] = daily[name+'_max']/daily[name+'_min']\n",
    "    daily[name+'_std'] = daily[[name+'_close', name+'_open', name+'_max', name+'_min']].std(axis = 1)\n",
    "    daily[name+'_delta_2'] = daily[name]/daily[name].shift(2)\n",
    "    daily[name+'_delta_10'] = daily[name]/daily[name].shift(10)\n",
    "    daily[name+'_delta_30'] = daily[name]/daily[name].shift(30)\n",
    "    daily[name+'_delta_60'] = daily[name]/daily[name].shift(60)   \n",
    "    \n",
    "\n",
    "for col in ['paraxylene_CHN_USD', 'paraxylene_RT_USD', 'paraxylene_SEA_USD', 'pta_NEA_USD', 'ethylene_glycol_EU_EUR', 'ethylene_glycol_CHN_USD']:\n",
    "    weekly[col+'_delta_7'] = weekly[col]/weekly[col].shift(1)\n",
    "    weekly[col+'_delta_14'] = weekly[col]/weekly[col].shift(2)\n",
    "    weekly[col+'_delta_28'] = weekly[col]/weekly[col].shift(4)\n",
    "    weekly[col+'_delta_56'] = weekly[col]/weekly[col].shift(8)\n",
    "    \n",
    "weekly['paraxylene_diff1'] = weekly['paraxylene_CHN_USD']/weekly['paraxylene_RT_USD']\n",
    "weekly['paraxylene_diff2'] = weekly['paraxylene_CHN_USD']/weekly['paraxylene_SEA_USD']\n",
    "weekly['paraxylene_diff3'] = weekly['paraxylene_RT_USD']/weekly['paraxylene_SEA_USD']\n",
    "weekly['ethylene_glycol_diff1'] = weekly['ethylene_glycol_EU_EUR']/weekly['ethylene_glycol_CHN_USD']\n",
    "\n",
    "\n",
    "dfts = daily.resample(\"D\").ffill()\n",
    "wfts = weekly.resample(\"D\").ffill()\n",
    "fts = dfts.join(wfts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляю к нагенеренным данным всевозможные исходные даынне тем же образом образом, что и нагенеренные фичи\n",
    "\n",
    "data = fts\n",
    "data = data[data.index.map(lambda x: x.day == 9)]\n",
    "data.index = data.index.map(lambda x: pd.to_datetime('-'.join([str(x.year), str(x.month), '1'])))\n",
    "\n",
    "data.loc[pd.to_datetime('2019-07-01')] = None\n",
    "\n",
    "test = test.join(data.shift(1))\n",
    "train = train.join(data.shift(1))\n",
    "\n",
    "x, y = train.drop(['pet'],axis = 1), train['pet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обрабатываю данные, тк модель линейная\n",
    "\n",
    "info = pd.DataFrame()\n",
    "\n",
    "columns = x.columns\n",
    "cleaned_x = x[columns].fillna(method = 'pad').fillna(x.mean())\n",
    "\n",
    "cleaned_x = pd.DataFrame(np.nan_to_num(cleaned_x), columns = cleaned_x.columns, index = cleaned_x.index)\n",
    "\n",
    "\n",
    "all_columns = cleaned_x.columns\n",
    "cleaned_x[~np.isfinite(cleaned_x.astype(np.float32))] = np.finfo(np.float32).max\n",
    "\n",
    "cleaned_test_x = test[columns].fillna(method = 'pad').fillna(x.mean())\n",
    "\n",
    "cleaned_test_x = pd.DataFrame(np.nan_to_num(cleaned_test_x), columns = cleaned_test_x.columns, index = cleaned_test_x.index)\n",
    "\n",
    "\n",
    "all_columns = cleaned_test_x.columns\n",
    "cleaned_test_x[~np.isfinite(cleaned_test_x.astype(np.float32))] = np.finfo(np.float32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cleaned_x  = cleaned_x\n",
    "start_cleaned_test_x  = cleaned_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh_columns = list(tsfresh.select_features(start_cleaned_x,  y, fdr_level=0.0000001).columns)\n",
    "\n",
    "cleaned_x = start_cleaned_x[tsfresh_columns]\n",
    "cleaned_test_x = start_cleaned_test_x[tsfresh_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "model = LinearSVR(C=0.032, random_state =1)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "model.fit(scaler.fit_transform(cleaned_x), y)\n",
    "ts_preds = model.predict(scaler.transform(cleaned_test_x))\n",
    "\n",
    "ts_preds = pd.DataFrame(ts_preds, columns=[\"pet\"], index=cleaned_test_x.index)\n",
    "ts_preds.to_csv(DATA_DIR.joinpath(\"pet.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
